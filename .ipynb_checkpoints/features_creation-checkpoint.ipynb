{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith open(rootdir+\"/\"+file, 'r', endoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_igor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data cleaning (Igor):\n",
    "- Remove all the HTML symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove HLML signs\n",
    "HTMLlist = ['<br />']\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    for word in HTMLlist:\n",
    "        reviews[idx] = review.replace(word,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features : Other ideas to try\n",
    "\n",
    "- Find N-grams where it may start with a CAPITAL (As for the movie names and actor's names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature creation (Sofia):\n",
    "- List punctuation (various form)\n",
    "- Find smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla = [0]*len(reviews)\n",
    "inter = [0]*len(reviews)\n",
    "susp = [0]*len(reviews)\n",
    "for i, review in enumerate(reviews):\n",
    "    for char in review:\n",
    "        if char == \"?\":\n",
    "            inter[i] += 1\n",
    "        elif char == \"!\":\n",
    "            excla[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "ttoken = TweetTokenizer(reduce_len=True)\n",
    "tokenized_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    tokenized_reviews.append(ttoken.tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, review in enumerate(tokenized_reviews):\n",
    "    for word in review:\n",
    "        if word == \"...\":\n",
    "            susp[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature creation (Igor): \n",
    "- Length of the review\n",
    "- Grad given in the review\n",
    "- Movie mentionned in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The movie list has to be completed\n",
    "# - Find a list of the movie so as it matches some in the reviews : www.imdb.com\n",
    "movie_list = ['Titanic']\n",
    "\n",
    "rev_length = []\n",
    "rev_word_count = []\n",
    "rev_movie = []\n",
    "\n",
    "for idr,review in enumerate(reviews):\n",
    "    # Length of the review\n",
    "    rev_length.append(len(review))\n",
    "    \n",
    "    rev_word_count.append(0)\n",
    "    for word in review:\n",
    "        rev_word_count[idr]+=1\n",
    "        \n",
    "    # Movie in the review\n",
    "    for movie_name in movie_list:\n",
    "        if movie_name in review:\n",
    "            rev_movie.append(movie_name)\n",
    "            #break\n",
    "        else:\n",
    "            rev_movie.append('no movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_grade = []\n",
    "# Grade/Mark in the review\n",
    "for idr,review in enumerate(reviews):\n",
    "    rev_grade.append([])\n",
    "    review_split= review.split(\" \")\n",
    "\n",
    "    for idw, word in enumerate(review_split):\n",
    "        for idx, char in enumerate(word):\n",
    "                if char == '/':\n",
    "                    ten_is_there= False\n",
    "                    if(idx < len(word)-2):\n",
    "                        if word[idx+1] == '1' and word[idx+2] == '0':\n",
    "                            ten_is_there=True\n",
    "                    if(idx== len(word) -1 and idw<len(review_split)-1 and len(review_split[idw+1])>1 ):\n",
    "                        if(review_split[idw+1][0]=='1' and review_split[idw+1][1]=='0'):\n",
    "                            ten_is_there=True\n",
    "                    if(ten_is_there):                  \n",
    "                        if(idx)>0:\n",
    "                            rev_grade[idr].append(word[0:idx])\n",
    "                        else:\n",
    "                            rev_grade[idr].append(review_split[idw-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rev_grade' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1e361a69e8a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mnew_rev_grade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrade\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev_grade\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrade\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rev_grade' is not defined"
     ]
    }
   ],
   "source": [
    "# function for reviews with one grade\n",
    "def convert_to_real_grade(grade):\n",
    "    try:\n",
    "        return float(grade)\n",
    "    except:\n",
    "        good = '0123465789'\n",
    "        if grade[-1] not in good:\n",
    "            return -1000\n",
    "        one_dot = False\n",
    "        new_grade = ''\n",
    "        for char in reversed(grade):\n",
    "            if char in good:\n",
    "                new_grade = char + new_grade\n",
    "            elif char in '.,/' and one_dot == False:\n",
    "                new_grade  = '.' + new_grade\n",
    "                one_dot = True\n",
    "            else:\n",
    "                if new_grade[0] in '.,/':\n",
    "                    print grade, '  -  ', new_grade[1]\n",
    "                    return float(new_grade[1])\n",
    "                else:\n",
    "                    print grade, '  -  ', new_grade\n",
    "                    return float(new_grade)\n",
    "        return -1000\n",
    "                \n",
    "                    \n",
    "\n",
    "# functions for reviews with more than one grade\n",
    "def convert_to_real_grade_2(grade):\n",
    "    new_grades = []\n",
    "    for i in range(len(grade)):\n",
    "        new_grade = convert_to_real_grade(grade[i])\n",
    "        new_grades.append(new_grade)\n",
    "    #return new_grades\n",
    "    \n",
    "\n",
    "new_rev_grade = []\n",
    "for idg, grade in enumerate(rev_grade):\n",
    "    if grade != []:\n",
    "        if len(grade) == 1:\n",
    "            new_rev_grade.append(convert_to_real_grade(grade[0]))\n",
    "        else:\n",
    "            new_rev_grade.append(convert_to_real_grade_2(grade))\n",
    "    else: \n",
    "        new_rev_grade.append(-1.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Data Cleaning (After the features creation) :\n",
    " - Punctuation\n",
    " - Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove punctuation, lower all characters\n",
    "# exclude = {',' ,'+', '<', ':', '/', ']', '(', ')', '{', '\"', '_', '?', '@', '}', ...}\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w for w in review.lower() if w not in exclude])\n",
    "    \n",
    "    \n",
    "# Remove stop words based on the given list - To be changed depending on the needs\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w for w in review if w not in stopwords])\n",
    "    #problème de codage à voir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Data Cleaning (Baptiste): \n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e59170c14185>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Igor\\Anaconda\\envs\\py27\\lib\\site-packages\\nltk\\stem\\snowball.pyc\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;31m# STEP 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__step4_suffixes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ion\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Steeming -> Reduce words to their initial mining\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    for word in review: \n",
    "        reviews[idx] = ''.join([stemmer.stem(word)+' ' for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - Idf Matrix (Igor)\n",
    "\n",
    "#### To be upgraded with new tf and idf functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features extraction with TF - IDF : get the matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "m = TfidfVectorizer()\n",
    "tfidf_matrix = m.fit_transform(reviews)\n",
    "\n",
    "print(\"Size of the tfidf matrix: \", tfidf_matrix.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new features to the Tf-Idf matrix\n",
    "#### New features are : \n",
    "- Number of exclamation point\n",
    "- Number of interrogation point\n",
    "- Number of suspension point\n",
    "- Review length\n",
    "- Number of word (word_count)\n",
    "- Movie mentionned\n",
    "- Grade mentionned\n",
    "\n",
    "- Smileys (How to deal with them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b):\n",
    "    \"\"\" Takes in 2 csr_matrices and appends the second one to the bottom of the first one. \n",
    "    Much faster than scipy.sparse.vstack but assumes the type to be csr and overwrites\n",
    "    the first matrix instead of copying it. The data, indices, and indptr still get copied.\"\"\"\n",
    "\n",
    "    a.data = np.hstack((a.data,b.data))\n",
    "    a.indices = np.hstack((a.indices,b.indices))\n",
    "    a.indptr = np.hstack((a.indptr,(b.indptr + a.nnz)[1:]))\n",
    "    a._shape = (a.shape[0]+b.shape[0],b.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla1 = scipy.sparse.csr_matrix(excla)\n",
    "inter1 = scipy.sparse.csr_matrix(inter)\n",
    "susp1 = scipy.sparse.csr_matrix(susp)\n",
    "rev_length1 = scipy.sparse.csr_matrix(rev_length)\n",
    "rev_word_count1 = scipy.sparse.csr_matrix(rev_word_count)\n",
    "rev_grade1 = scipy.sparse.csr_matrix(new_rev_grade)\n",
    "rev_movie1 = scipy.sparse.csr_matrix(np.array(rev_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csr_vappend(tfidf_matrix, excla1)\n",
    "csr_vappend(tfidf_matrix, inter1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CSV Creation (Baptiste):\n",
    "\n",
    "Create 5 csv : train_train.csv, train_test.csv, y_train_train.csv, y_train_test.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tf-idf matrix into two data sets to process the cross validation : training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(tfidf_matrix, Class, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [float(row[0]) for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train', data_train)\n",
    "save_sparse_csr('data_test', data_test)\n",
    "save_csv('label_train.csv', label_train)\n",
    "save_csv('label_test.csv', label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17500x78042 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 2073379 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<32500x78042 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 3835155 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x78042 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4716043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x78042 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 880888 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
