{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith open(rootdir+\"/\"+file, 'r', endoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_igor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data cleaning:\n",
    "- Remove all the HTML symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove HLML signs\n",
    "HTMLlist = ['<br />']\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    for word in HTMLlist:\n",
    "        reviews[idx] = review.replace(word,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature creation:\n",
    "- List punctuation (various form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla = [0]*len(reviews)\n",
    "inter = [0]*len(reviews)\n",
    "susp = [0]*len(reviews)\n",
    "for i, review in enumerate(reviews):\n",
    "    for char in review:\n",
    "        if char == \"?\":\n",
    "            inter[i] += 1\n",
    "        elif char == \"!\":\n",
    "            excla[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "ttoken = TweetTokenizer(reduce_len=True)\n",
    "tokenized_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    tokenized_reviews.append(ttoken.tokenize(review))\n",
    "    \n",
    "for i, review in enumerate(tokenized_reviews):\n",
    "    for word in review:\n",
    "        if word == \"...\":\n",
    "            susp[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature creation : \n",
    "- Length of the review\n",
    "- Movie mentionned in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_length = []\n",
    "rev_word_count = []\n",
    "\n",
    "\n",
    "for idr,review in enumerate(reviews):\n",
    "    # Length of the review\n",
    "    rev_length.append(len(review))\n",
    "    \n",
    "    words = 0\n",
    "    for word in review:\n",
    "        words +=1\n",
    "    rev_word_count.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THE MOVIE LIST HAS TO BE COMPLETED\n",
    "from imdbpie import Imdb\n",
    "imdb = Imdb(anonymize = True)\n",
    "\n",
    "# Movie list creation\n",
    "movie_list = {}\n",
    "for movie in imdb.top_250():\n",
    "    movie_list[movie['title']] = movie['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_movie = []\n",
    "wrong_titles = ['M', 'Up', 'Ran']\n",
    "\n",
    "for idr,review in enumerate(reviews):   \n",
    "    # Movie in the review\n",
    "    movies = []\n",
    "    for key, value in movie_list.items():\n",
    "        if key in review and key not in wrong_titles:\n",
    "            movies.append(value)\n",
    "    rev_movie.append(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TO BE DISCUSSED : WHAT IF MORE THAN 1 MOVIE MENTIONNED?\n",
    "new_rev_movie = []\n",
    "\n",
    "for movie in rev_movie:\n",
    "    # No movies quoted or more than 1 movie quoted\n",
    "    if len(movie) != 1:\n",
    "        new_rev_movie.append(-5.4321)\n",
    "    \n",
    "    # Only one movie quoted\n",
    "    else:\n",
    "        new_rev_movie.append(movie[0])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AS FOR NOW, THERE ARE ONLY BAD MOVIES\n",
    "good_movie_mentionned = []\n",
    "bad_movie_mentionned = []\n",
    "\n",
    "for movie in new_rev_movie:\n",
    "    if movie > 6.8:\n",
    "        good_movie_mentionned.append(True)\n",
    "        bad_movie_mentionned.append(False)\n",
    "    elif movie < 4.0:\n",
    "        good_movie_mentionned.append(False)\n",
    "        bad_movie_mentionned.append(True)\n",
    "    else:\n",
    "        good_movie_mentionned.append(False)\n",
    "        bad_movie_mentionned.append(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation :\n",
    "- Grade mentionned in the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_grade = []\n",
    "# Grade/Mark in the review\n",
    "for idr,review in enumerate(reviews):\n",
    "    rev_grade.append([])\n",
    "    review_split= review.split(\" \")\n",
    "\n",
    "    for idw, word in enumerate(review_split):\n",
    "        for idx, char in enumerate(word):\n",
    "                if char == '/':\n",
    "                    ten_is_there= False\n",
    "                    if(idx < len(word)-2):\n",
    "                        if word[idx+1] == '1' and word[idx+2] == '0':\n",
    "                            ten_is_there=True\n",
    "                    if(idx== len(word) -1 and idw<len(review_split)-1 and len(review_split[idw+1])>1 ):\n",
    "                        if(review_split[idw+1][0]=='1' and review_split[idw+1][1]=='0'):\n",
    "                            ten_is_there=True\n",
    "                    if(ten_is_there):                  \n",
    "                        if(idx)>0:\n",
    "                            rev_grade[idr].append(word[0:idx])\n",
    "                        else:\n",
    "                            rev_grade[idr].append(review_split[idw-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THE PRINT NOW : The function may be better when used on the test set\n",
    "\n",
    "# SOMETIMES : 1/10/2015 -> It is a date ! \n",
    "\n",
    "def convert_to_real_grade(grade):\n",
    "    new_grade = 5.4321\n",
    "    \n",
    "    ### The grade is a float\n",
    "    try:\n",
    "        new_grade = float(grade)\n",
    "        return float(new_grade)\n",
    "    \n",
    "    ### The grade is not a float\n",
    "    except:\n",
    "        good = '0123465789'\n",
    "        numerical_words = {'zero':0, 'one':1, 'two':1, 'three':3, 'four':4, 'five':5, \n",
    "                           'six':6, 'seven':7, 'height':8, 'nine':9, 'ten':10}\n",
    "        \n",
    "        ## The grade has numerical values at the end \n",
    "        if grade[-1] in good:\n",
    "\n",
    "            ### Read the grade in the string\n",
    "            one_dot = False\n",
    "            g_new = ''\n",
    "            for char in reversed(grade):\n",
    "                if char in good:\n",
    "                    g_new = char + g_new\n",
    "                elif char in '.,' and one_dot == False:\n",
    "                    g_new  = '.' + g_new\n",
    "                    one_dot = True\n",
    "                else:\n",
    "                    if g_new[0] in '.,':\n",
    "                        new_grade = g_new[1:]\n",
    "                    else:\n",
    "                        new_grade = g_new\n",
    "                    \n",
    "        elif (grade[-1] not in good) and (grade.lower() in numerical_words):\n",
    "            new_grade = numerical_words[grade.lower()]\n",
    "            \n",
    "        return float(new_grade)\n",
    "                \n",
    "                    \n",
    "\n",
    "# functions for reviews with more than one grade\n",
    "def convert_to_real_grade_2(grade):\n",
    "    final_grade = 5.4321\n",
    "    new_grades = []\n",
    "    for i in range(len(grade)):\n",
    "        new_grade = convert_to_real_grade(grade[i])\n",
    "        new_grades.append(new_grade)\n",
    "        \n",
    "    if final_grade not in new_grades:\n",
    "        ## NEXT CONDITION IS TO BE DISCUSSED\n",
    "        if np.max(new_grades) - np.min(new_grades) < 7:\n",
    "            final_grade = np.mean(new_grades)\n",
    "    return final_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rev_grade = []\n",
    "for idg, grade in enumerate(rev_grade):    \n",
    "    converted_grade = 5.4321\n",
    "    \n",
    "    if grade != []:\n",
    "        if len(grade) == 1:\n",
    "            converted_grade = convert_to_real_grade(grade[0])\n",
    "        else:\n",
    "            converted_grade = convert_to_real_grade_2(grade)\n",
    "    new_rev_grade.append(converted_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dummies\n",
    "good_grade = []\n",
    "bad_grade = []\n",
    "\n",
    "for grade in new_rev_grade:\n",
    "    if grade > 6.8:\n",
    "        good_grade.append(True)\n",
    "        bad_grade.append(False)\n",
    "    elif grade <4.0:\n",
    "        good_grade.append(False)\n",
    "        bad_grade.append(True)\n",
    "    else:\n",
    "        good_grade.append(False)\n",
    "        bad_grade.append(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features : Other ideas to try\n",
    "\n",
    "- Find N-grams where it may start with a CAPITAL (As for the movie names and actor's names)\n",
    "- Add smileys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = [\":-)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":-))\", \"^^\"]\n",
    "laughing = [\":-D\", \"8-D\", \"8D\", \"x-D\", \"xD\", \"X-D\", \"XD\", \"=-D\", \"=D\", \"=-3\", \"=3\", \"B^D\"]\n",
    "sad = [\">:[\", \":-(\", \":(\", \":-c\", \":c\", \":-<\", \":<\", \":-[\", \":[\", \":{\", \";(\"]\n",
    "cry = [\":'-(\", \":'(\"]\n",
    "happy_cry = [\":'-)\", \":')\"]\n",
    "horror = [\"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D-':\"]\n",
    "surprised = [\">:O\", \":-O\", \":O\", \":-o\", \":o\", \"8-0\", \"O_O\", \"o-o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\"]\n",
    "kiss= [\":*\", \":^*\", \"( '}{' )\"]\n",
    "wink = [\";-)\", \";)\", \"*-)\", \"*)\", \";-]\", \";]\", \";D\", \";^)\", \":-\"]\n",
    "tongue = [\">:P\", \":-P\", \":P\", \"X-P\", \"x-p\", \"xp\", \"XP\", \":-p\", \":p\", \"=p\", \":-b\", \":b\", \"d:\"]\n",
    "skeptical = [\">:\\ \".replace(\" \", \"\"), \">:/\", \":-/\", \":-.\", \":/\", \":\\ \".replace(\" \", \"\"), \"=/\", \"=\\ \".replace(\" \", \"\"), \":L\", \"=L\", \":S\", \">.<\"]\n",
    "neutral = [\":|\", \":-|\"]\n",
    "angel = [\"O:-)\", \"0:-3\", \"0:3\", \"0:-)\", \"0:)\", \"0;^)\"]\n",
    "evil = [\">:)\", \">;)\", \">:-)\", \"}:-)\", \"}:)\", \"3:-)\", \"3:)\"]\n",
    "high_five = [\"o/\\o\", \"^5\", \">_>^ ^<_<\"]\n",
    "heart = [\"<3\"]\n",
    "broken_hart = [\"</3\"]\n",
    "angry = [\":@\"]\n",
    "smiley_list = [\n",
    "happy,\n",
    "laughing,\n",
    "sad,\n",
    "cry,\n",
    "happy_cry,\n",
    "horror,\n",
    "surprised,\n",
    "kiss,\n",
    "wink,\n",
    "tongue,\n",
    "skeptical,\n",
    "neutral,\n",
    "angel,\n",
    "evil,\n",
    "high_five,\n",
    "heart,\n",
    "broken_hart, \n",
    "angry]\n",
    "smiley_names = [\n",
    "\"happy\",\n",
    "\"laughing\",\n",
    "\"sad\",\n",
    "\"cry\",\n",
    "\"happy_cry\",\n",
    "\"horror\",\n",
    "\"surprised\",\n",
    "\"kiss\",\n",
    "\"wink\",\n",
    "\"tongue\",\n",
    "\"skeptical\",\n",
    "\"neutral\",\n",
    "\"angel\",\n",
    "\"evil\",\n",
    "\"high_five\",\n",
    "\"heart\",\n",
    "\"broken_hart\", \n",
    "\"angry\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_features_smiley(tokenized_text, smiley_list):\n",
    "    features_smiley = np.zeros((len(tokenized_text),len(smiley_list)))\n",
    "    for i, review in enumerate(tokenized_text):\n",
    "        for w in review :\n",
    "            if len(w)<2 : \n",
    "                pass\n",
    "            elif len(w)>5:\n",
    "                pass\n",
    "            for j, cat in enumerate(smiley_list):\n",
    "                if w in cat:\n",
    "                    features_smiley[i,j] = 1\n",
    "    return features_smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_smiley = gen_features_smiley(tokenized_reviews, smiley_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Data Cleaning (After the features creation) :\n",
    " - Punctuation\n",
    " - Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove punctuation, lower all characters\n",
    "# exclude = {',' ,'+', '<', ':', '/', ']', '(', ')', '{', '\"', '_', '?', '@', '}', ...}\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w if w not in exclude else \" \" for w in review.lower() ])\n",
    "    \n",
    "# Remove stop words based on the given list - To be changed depending on the needs\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w +' ' for w in review.split() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Data Cleaning: \n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steeming -> Reduce words to their initial mining\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([stemmer.stem(word)+' ' for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - Idf Matrix\n",
    "\n",
    "#### To be upgraded with new tf and idf functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features extraction with TF - IDF : get the matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "m = TfidfVectorizer()\n",
    "tfidf_matrix = m.fit_transform(reviews)\n",
    "\n",
    "print(\"Size of the tfidf matrix: \", tfidf_matrix.size)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new features to the Tf-Idf matrix\n",
    "#### New features are : \n",
    "- Number of exclamation point\n",
    "- Number of interrogation point\n",
    "- Number of suspension point\n",
    "- Review length\n",
    "- Number of word (word_count)\n",
    "- Movie mentionned\n",
    "- Grade mentionned\n",
    "\n",
    "- Smileys (How to deal with them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b): #b est un vecteur ligne (np.array ou liste) et a est une sparse matrix\n",
    "    if(type(a)!= scipy.sparse.csr.csr_matrix):\n",
    "        a=scipy.sparse.csr_matrix(a)\n",
    "        \n",
    "    if(type(b)== list):\n",
    "        b=np.array([b]).T\n",
    "    if(type(b)!= scipy.sparse.csr.csr_matrix):\n",
    "        b=scipy.sparse.csr_matrix(b)\n",
    "       \n",
    "    a=scipy.sparse.hstack([a,b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csr_vappend(tfidf_matrix, excla)\n",
    "csr_vappend(tfidf_matrix, inter)\n",
    "csr_vappend(tfidf_matrix, susp)\n",
    "csr_vappend(tfidf_matrix, rev_length)\n",
    "csr_vappend(tfidf_matrix, rev_word_count)\n",
    "csr_vappend(tfidf_matrix, good_movie)\n",
    "csr_vappend(tfidf_matrix, bad_movie)\n",
    "csr_vappend(tfidf_matrix, good_movie_mentionned)\n",
    "csr_vappend(tfidf_matrix, bad_movie_mentionned)\n",
    "csr_vappend(tfidf_matrix, features_smiley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CSV Creation:\n",
    "\n",
    "Create 5 csv : train_train.csv, train_test.csv, y_train_train.csv, y_train_test.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tf-idf matrix into two data sets to process the cross validation : training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(tfidf_matrix, Class, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [float(row[0]) for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train', data_train)\n",
    "save_sparse_csr('data_test', data_test)\n",
    "save_csv('label_train.csv', label_train)\n",
    "save_csv('label_test.csv', label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
