{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_igor_test = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/test\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUnknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdir+\"/\"+file, 'r', encoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_igor)\n",
    "test_test_reviews, names = loadUnknown(path_igor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions\n",
    "- HTML symbols\n",
    "- Punctuation and StopWords\n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_HTML(reviews):\n",
    "    HTMLwords = ['<br />']\n",
    "    \n",
    "    for idx, review in enumerate(reviews):\n",
    "        for word in HTMLwords:\n",
    "            reviews[idx] = review.replace(word,'')\n",
    "   \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "            \n",
    "def clean_stopwords_and_punctuation(reviews):\n",
    "    exclude = set(string.punctuation)\n",
    "    stopword = stopwords.words(\"english\")\n",
    "    \n",
    "    for idx, review  in enumerate(reviews):\n",
    "        reviews[idx] =  ''.join([w.lower() + ' ' if w not in exclude else '' for w in review.split()])\n",
    "        reviews[idx] = ''.join([w + ' ' if w not in stopword else ''  for w in review.split()])\n",
    "     \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stemmisation(reviews):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for idx, review in enumerate(reviews):\n",
    "        reviews[idx] =  ''.join([stemmer.stem(word) for word in review])\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation\n",
    "- Exclamation (!), interrogation (?), length, number of words\n",
    "- Suspension (...)\n",
    "- Smileys mentionned\n",
    "- Movie mentionned\n",
    "- Grade mentionned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creation_excla_inter_length_wordcount(reviews):\n",
    "    excla = [w.count('!') for w in reviews]\n",
    "    inter = [w.count('?') for w in reviews]\n",
    "    length = [len(w) for w in reviews]\n",
    "    wordcount = [len(w.split()) for w in reviews]\n",
    "    \n",
    "    return excla, inter, length, wordcount            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = [\":-)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":-))\", \"^^\"]\n",
    "laughing = [\":-D\", \"8-D\", \"8D\", \"x-D\", \"xD\", \"X-D\", \"XD\", \"=-D\", \"=D\", \"=-3\", \"=3\", \"B^D\"]\n",
    "sad = [\">:[\", \":-(\", \":(\", \":-c\", \":c\", \":-<\", \":<\", \":-[\", \":[\", \":{\", \";(\"]\n",
    "cry = [\":'-(\", \":'(\"]\n",
    "happy_cry = [\":'-)\", \":')\"]\n",
    "horror = [\"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D-':\"]\n",
    "surprised = [\">:O\", \":-O\", \":O\", \":-o\", \":o\", \"8-0\", \"O_O\", \"o-o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\"]\n",
    "kiss= [\":*\", \":^*\", \"( '}{' )\"]\n",
    "wink = [\";-)\", \";)\", \"*-)\", \"*)\", \";-]\", \";]\", \";D\", \";^)\", \":-\"]\n",
    "tongue = [\">:P\", \":-P\", \":P\", \"X-P\", \"x-p\", \"xp\", \"XP\", \":-p\", \":p\", \"=p\", \":-b\", \":b\", \"d:\"]\n",
    "skeptical = [\">:\\ \".replace(\" \", \"\"), \">:/\", \":-/\", \":-.\", \":/\", \":\\ \".replace(\" \", \"\"), \"=/\", \"=\\ \".replace(\" \", \"\"), \":L\", \"=L\", \":S\", \">.<\"]\n",
    "neutral = [\":|\", \":-|\"]\n",
    "angel = [\"O:-)\", \"0:-3\", \"0:3\", \"0:-)\", \"0:)\", \"0;^)\"]\n",
    "evil = [\">:)\", \">;)\", \">:-)\", \"}:-)\", \"}:)\", \"3:-)\", \"3:)\"]\n",
    "high_five = [\"o/\\o\", \"^5\", \">_>^ ^<_<\"]\n",
    "heart = [\"<3\"]\n",
    "broken_hart = [\"</3\"]\n",
    "angry = [\":@\"]\n",
    "smiley_list = [\n",
    "happy, laughing, sad, cry, happy_cry, horror, surprised, kiss, wink, tongue, skeptical, neutral, angel, evil, high_five, heart, broken_hart, angry]\n",
    "smiley_names = [ \"happy\", \"laughing\", \"sad\", \"cry\", \"happy_cry\", \"horror\", \"surprised\", \"kiss\", \"wink\", \"tongue\", \"skeptical\", \"neutral\", \"angel\", \"evil\", \"high_five\", \"heart\", \"broken_hart\",  \"angry\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "def creation_suspension_and_smiley(reviews, smiley_list):\n",
    "    ttoken = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    tokenized_reviews =  [ttoken.tokenize(w) for w in reviews]    \n",
    "    susp = [w.count('...') for w in tokenized_reviews]\n",
    "    \n",
    "    smileys = []\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        for w in review :\n",
    "            if len(w)<2 or len(w) > 5 : \n",
    "                pass\n",
    "            smiley_in_review = [1 if w in cat else 0 for cat in smiley_list]\n",
    "        smileys.append(smiley_in_review)\n",
    "                \n",
    "    return susp, np.array(smileys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get anything being before \" /10\" or \"over ten\" or between both\n",
    "def get_all_mentionned_grade(reviews):\n",
    "    rev_grade = []\n",
    "    for idr,review in enumerate(reviews):\n",
    "        rev_grade.append([])\n",
    "        review_split= review.split()\n",
    "\n",
    "        for idw, word in enumerate(review_split):\n",
    "            try:\n",
    "                if((word==\"on\" or word==\"over\") and idw+1<len(review_split)):\n",
    "                    if(review_split[idw+1][0:3]==\"ten\" or review_split[idw+1][0:2]==\"10\"):\n",
    "                        if(idw>0):\n",
    "                             rev_grade[idr].append(review_split[idw-1])\n",
    "            except: print review_split\n",
    "\n",
    "            for idx, char in enumerate(word):\n",
    "                    if char == '/':\n",
    "                        ten_is_there= False\n",
    "                        if(idx < len(word)-2):\n",
    "                            if word[idx+1] == '1' and word[idx+2] == '0':\n",
    "                                ten_is_there=True\n",
    "                        if(idx < len(word)-3):\n",
    "                            if word[idx+1] == 't' and word[idx+2] == 'e' and word[idx+3]==\"n\":\n",
    "                                ten_is_there=True\n",
    "                        if(idx== len(word) -1 and idw<len(review_split)-1 and len(review_split[idw+1])>1 ):\n",
    "                            if((review_split[idw+1][0]=='1' and review_split[idw+1][1]=='0') or review_split[idw+1][0:3]==\"ten\"):\n",
    "                                ten_is_there=True\n",
    "\n",
    "                        if(ten_is_there):                  \n",
    "                            if(idx)>0:\n",
    "                                rev_grade[idr].append(word[0:idx])\n",
    "                            else:\n",
    "                                if(idw>0):\n",
    "                                    rev_grade[idr].append(review_split[idw-1])\n",
    "    return rev_grade\n",
    "\n",
    "\n",
    "# Convert what have been collected to a grade. If impossible, then the grade is 5.4321\n",
    "def convert_to_real_grade(grade):\n",
    "    new_grade = 5.4321\n",
    "    \n",
    "    ### The grade is a float\n",
    "    try:\n",
    "        new_grade = float(grade)\n",
    "        return float(new_grade)\n",
    "    \n",
    "    ### The grade is not a float\n",
    "    except:\n",
    "        good = '0123465789'\n",
    "        numerical_words = {'zero':0, 'one':1, 'two':1, 'three':3, 'four':4, 'five':5, \n",
    "                           'six':6, 'seven':7, 'height':8, 'nine':9, 'ten':10}\n",
    "        \n",
    "        ## The grade has numerical values at the end \n",
    "        if grade[-1] in good:\n",
    "\n",
    "            ### Read the grade in the string\n",
    "            one_dot = False\n",
    "            g_new = ''\n",
    "            for char in reversed(grade):\n",
    "                if char in good:\n",
    "                    g_new = char + g_new\n",
    "                elif char in '.,' and one_dot == False:\n",
    "                    g_new  = '.' + g_new\n",
    "                    one_dot = True\n",
    "                else:\n",
    "                    if g_new[0] in '.,':\n",
    "                        new_grade = g_new[1:]\n",
    "                    else:\n",
    "                        new_grade = g_new\n",
    "                    \n",
    "        elif (grade[-1] not in good) and (grade.lower() in numerical_words):\n",
    "            new_grade = numerical_words[grade.lower()]\n",
    "            \n",
    "        return float(new_grade)\n",
    "    \n",
    "\n",
    "# Same as previous but in the case that the reviews mentionned many grades\n",
    "def convert_to_real_grade_2(grade):\n",
    "    final_grade = 5.4321\n",
    "    new_grades = []\n",
    "    for i in range(len(grade)):\n",
    "        new_grade = convert_to_real_grade(grade[i])\n",
    "        new_grades.append(new_grade)\n",
    "        \n",
    "    if final_grade not in new_grades:\n",
    "        ## NEXT CONDITION IS TO BE DISCUSSED\n",
    "        if np.max(new_grades) - np.min(new_grades) < 7:\n",
    "            final_grade = np.mean(new_grades)\n",
    "    return final_grade\n",
    "\n",
    "\n",
    "# Return a single grade for each review\n",
    "def creation_grade(reviews):\n",
    "    rev_grade = get_all_mentionned_grade(reviews)\n",
    "    new_rev_grade = []\n",
    "    \n",
    "    for idg, grade in enumerate(rev_grade):    \n",
    "        converted_grade = 5.4321\n",
    "\n",
    "        if grade != []:\n",
    "            if len(grade) == 1:\n",
    "                converted_grade = convert_to_real_grade(grade[0])\n",
    "            else:\n",
    "                converted_grade = convert_to_real_grade_2(grade)\n",
    "        new_rev_grade.append(converted_grade)\n",
    "        \n",
    "    return new_rev_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return two OneHotEncoder (Or dummies)\n",
    "# The first says if it is a good movie\n",
    "# The second says if it is a bad movie\n",
    "def creation_good_grade_bad_grade(grades):\n",
    "    good_grade = []\n",
    "    bad_grade = []\n",
    "\n",
    "    for grade in grades:\n",
    "        if grade > 6.8:\n",
    "            good_grade.append(True)\n",
    "            bad_grade.append(False)\n",
    "        elif grade <4.0:\n",
    "            good_grade.append(False)\n",
    "            bad_grade.append(True)\n",
    "        else:\n",
    "            good_grade.append(False)\n",
    "            bad_grade.append(False)\n",
    "            \n",
    "    return good_grade, bad_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imdbpie import Imdb\n",
    "\n",
    "def good_and_bad_movies():\n",
    "    good_movies = {}\n",
    "    bad_movies = {}\n",
    "    \n",
    "    imdb = Imdb(anonymize = True)\n",
    "    for movie in imdb.top_250():\n",
    "        good_movies[movie['title']] = movie['rating']\n",
    "        \n",
    "    return good_movies, bad_movies\n",
    "\n",
    "def mentionned_movies(reviews):\n",
    "    good_movies, bad_movies = good_and_bad_movies()\n",
    "    wrong_titles = wrong_titles = ['M', 'Up', 'Ran']\n",
    "    \n",
    "    rev_movies = []\n",
    "    \n",
    "    for idr,review in enumerate(reviews):   \n",
    "        # Movie in the review\n",
    "        movies = []\n",
    "        for key, value in good_movies.items():\n",
    "            if key in review and key not in wrong_titles:\n",
    "                movies.append(value)\n",
    "        for key, value in bad_movies.items():\n",
    "            if key in review and key not in wrong_titles:\n",
    "                movies.append(value)\n",
    "                \n",
    "        rev_movies.append(movies)\n",
    "    \n",
    "    return rev_movies\n",
    "    \n",
    "def creation_good_bad_mentionned_movies(reviews):\n",
    "    list_mentionned_movies = mentionned_movies(reviews)\n",
    "    \n",
    "    good_movie_mentionned = []\n",
    "    bad_movie_mentionned = []\n",
    "\n",
    "    for movie in list_mentionned_movies:\n",
    "        if movie > 6.8:\n",
    "            good_movie_mentionned.append(True)\n",
    "            bad_movie_mentionned.append(False)\n",
    "        elif movie < 4.0:\n",
    "            good_movie_mentionned.append(False)\n",
    "            bad_movie_mentionned.append(True)\n",
    "        else:\n",
    "            good_movie_mentionned.append(False)\n",
    "            bad_movie_mentionned.append(False)\n",
    "            \n",
    "    return good_movie_mentionned, bad_movie_mentionned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the feature creation pipeline (depending on test or train):\n",
    "- Splitting train_train and test_train\n",
    "- Cleaning : HTML\n",
    "- Feature Creation : Excla, Inter, Length, WordCount, Susp, Smiley, Movie & Grade\n",
    "- Cleaning : Punctuation, Stopwords & Steemisation\n",
    "- TfIdf on the train only! \n",
    "- Transform the test according to the tfidf\n",
    "- Add the previously created features to the tfidf matrix\n",
    "- Store them\n",
    "\n",
    "### N.B. : At the end, no more splitting because the algorithm has to be learnt on all the data, and apply to the test: \n",
    "- train_train => train\n",
    "- train_test => test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_reviews, test_reviews, train_label, test_label = train_test_split(reviews, Class, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Cleaning HTML'\n",
    "train_reviews = clean_HTML(train_reviews)\n",
    "test_reviews = clean_HTML(test_reviews)\n",
    "test_test_reviews = clean_HTML(test_test_reviews)\n",
    "\n",
    "print 'Feature Creation : Excla, Inter, Length, Wordcount'\n",
    "train_excla, train_inter, train_length, train_wordcount = creation_excla_inter_length_wordcount(train_reviews)\n",
    "test_excla, test_inter, test_length, test_wordcount = creation_excla_inter_length_wordcount(test_reviews)\n",
    "\n",
    "print 'Feature Creation : Susp, Smileys'\n",
    "train_susp, train_smileys = creation_suspension_and_smiley(train_reviews, smiley_list)\n",
    "test_susp, test_smileys = creation_suspension_and_smiley(test_reviews, smiley_list)\n",
    "\n",
    "print 'Feature Creation : Good and bad movies mentionned'\n",
    "train_good_movies, train_bad_movies = creation_good_bad_mentionned_movies(train_reviews)\n",
    "test_good_movies, test_bad_movies = creation_good_bad_mentionned_movies(test_reviews)\n",
    "\n",
    "print 'Feature Creation : Good grades, bad grades'\n",
    "train_grades = creation_grade(train_reviews)\n",
    "train_good_grades, train_bad_grades = creation_good_grade_bad_grade(train_reviews)\n",
    "\n",
    "test_grades = creation_grade(test_reviews)\n",
    "test_good_grades, test_bad_grades = creation_good_grade_bad_grade(test_reviews)\n",
    "\n",
    "print 'Cleaning Stopwords & Punctuation'\n",
    "clean_stopwords_and_punctuation(train_reviews)\n",
    "clean_stopwords_and_punctuation(test_reviews)\n",
    "\n",
    "clean_stopwords_and_punctuation(test_test_reviews)\n",
    "\n",
    "print 'Stemmization'\n",
    "train_reviews = stemmisation(train_reviews)\n",
    "test_reviews = stemmisation(test_reviews)\n",
    "test_test_reviews = stemmisation(test_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Feature Creation : Susp, Smileys'\n",
    "train_susp, train_smileys = creation_suspension_and_smiley(train_reviews, smiley_list)\n",
    "test_susp, test_smileys = creation_suspension_and_smiley(test_reviews, smiley_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf matrix\n",
    "- Fit and transform on the train_reviews\n",
    "- Transform the test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transform and git on the training reviews\n",
    "#TO BE DISCUSSED : parameters : max_df, min_df, max_features\n",
    "ngram_range = (1,2)        # bigrams\n",
    "min_df = 5/25000 # considers terms that appears in more than 5 documents\n",
    "max_features = 750000    #considers only the top 10000 mono- and bi- grams ordered by term frequency across the corpus\n",
    "m = TfidfVectorizer(ngram_range = ngram_range, min_df = min_df, max_features = max_features)\n",
    "\n",
    "# Train the model with all the available data\n",
    "data = np.concatenate([train_reviews, test_reviews, test_test_reviews])\n",
    "train_tfidf = m.fit_transform(data)\n",
    "\n",
    "# Keep all the data in data & extract the first 18750 rows that are the real train_reviews\n",
    "data = train_tfidf\n",
    "train_tfidf = train_tfidf[:18750]\n",
    "\n",
    "# Transform the test_reviews\n",
    "test_tfidf = m.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data : \n",
    "- Concatenate the tfidf matrices and the new features\n",
    "- Get the name of the features\n",
    "- Storing the two matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b): #b est un vecteur ligne (np.array ou liste) et a est une sparse matrix\n",
    "    if(type(a)!= scipy.sparse.csr.csr_matrix):\n",
    "        a=scipy.sparse.csr_matrix(a)\n",
    "        \n",
    "    if(type(b)== list):\n",
    "        b=np.array([b]).T\n",
    "    if(type(b)!= scipy.sparse.csr.csr_matrix):\n",
    "        b=scipy.sparse.csr_matrix(b)\n",
    "        \n",
    "    return scipy.sparse.hstack([a,b], format ='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate for the train matrix\n",
    "train_tfidf = csr_vappend(train_tfidf, train_excla)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_inter)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_length)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_wordcount)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_susp)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_good_movies)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_bad_movies)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_smileys)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_good_grades)\n",
    "train_tfidf = csr_vappend(train_tfidf, train_bad_grades)\n",
    "\n",
    "\n",
    "# Concatenate for the test matrix\n",
    "test_tfidf = csr_vappend(test_tfidf, test_excla)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_inter)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_length)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_wordcount)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_susp)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_good_movies)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_bad_movies)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_smileys)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_good_grades)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_bad_grades)\n",
    "\n",
    "# Name of the features\n",
    "features = m.get_feature_names()\n",
    "features.append('excla')\n",
    "features.append('inter')\n",
    "features.append('susp')\n",
    "features.append('rev_length')\n",
    "features.append('rev_word_count')\n",
    "features.append('good_grade')\n",
    "features.append('bad_grade')\n",
    "features.append('good_movie_mentionned')\n",
    "features.append('bad_movie_mentionned')\n",
    "features.append('features_smiley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [row[0] for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train', data_train)\n",
    "save_sparse_csr('data_test', data_test)\n",
    "save_csv('label_train.csv', label_train)\n",
    "save_csv('label_test.csv', label_test)\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    features[i] = feat.encode('utf8', 'replace')\n",
    "save_csv('feature_names.csv', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_tfidf.shape, test_tfidf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
