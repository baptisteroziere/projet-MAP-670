{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "path_all = os.path.abspath(\"train\")\n",
    "path_all_test = os.path.abspath(\"test\") #works for me at least\n",
    "\n",
    "path_baptiste = \"../data/data_movies/train\"\n",
    "path_baptiste_test = \"../data/data_movies/test\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_igor_test = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/test\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUnknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdir+\"/\"+file, 'r', encoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_all)\n",
    "test_test_reviews, names = loadUnknown(path_all_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Functions\n",
    "- Lower\n",
    "- HTML symbols\n",
    "- Abbreviation\n",
    "- Punctuation \n",
    "- StopWords\n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lower(reviews):\n",
    "    for idx, review in enumerate(reviews):\n",
    "        reviews[idx] = review.lower()\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_HTML(reviews):\n",
    "    HTMLwords = ['<br />']\n",
    "    \n",
    "    for idx, review in enumerate(reviews):\n",
    "        for word in HTMLwords:\n",
    "            reviews[idx] = review.replace(word,' ')\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_abbreviation(reviews):\n",
    "    for idx, review in enumerate(reviews):\n",
    "        reviews[idx] = review.replace(\"'ll\", \" will\").replace(\"n't\", \" not\").replace(\"i'm\", \"i am\").replace(\"you're\", \"you are\").replace(\"wanna\", \"want to\").replace(\"gonna\", \"going to\")\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_punctuation(reviews):\n",
    "    exclude = set(string.punctuation)   \n",
    "    for idx, review  in enumerate(reviews):\n",
    "        reviews[idx] = ''.join([w.lower() if w not in exclude else ' ' for w in review])\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "        \n",
    "def clean_stopwords(reviews):\n",
    "    stopword = stopwords.words(\"english\") \n",
    "    for idx, review  in enumerate(reviews):\n",
    "        reviews[idx] = ''.join([w + ' ' if w.lower() not in stopword else ' '  for w in review.split()]) \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "def stemmisation(reviews):\n",
    "    stemmer = PorterStemmer()\n",
    "    for idx, review in enumerate(reviews):\n",
    "        reviews[idx] =  ' '.join([stemmer.stem(word) for word in review.split()])\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation\n",
    "- Exclamation (!), interrogation (?), length, number of words, suspension (...)\n",
    "- Smileys mentionned\n",
    "- Movie mentionned\n",
    "- Grade mentionned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creation_excla_inter_length_wordcount_susp(reviews):\n",
    "    excla = [w.count('!') for w in reviews]\n",
    "    inter = [w.count('?') for w in reviews]\n",
    "    length = [len(w) for w in reviews]\n",
    "    wordcount = [len(w.split()) for w in reviews]\n",
    "    susp = [w.count('...') for w in reviews]\n",
    "    \n",
    "    return np.array([excla, inter, length, wordcount, susp]).T         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = [\":-)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":-))\", \"^^\"]\n",
    "laughing = [\":-D\", \"8-D\", \"8D\", \"x-D\", \"xD\", \"X-D\", \"XD\", \"=-D\", \"=D\", \"=-3\", \"=3\", \"B^D\"]\n",
    "sad = [\">:[\", \":-(\", \":(\", \":-c\", \":c\", \":-<\", \":<\", \":-[\", \":[\", \":{\", \";(\"]\n",
    "cry = [\":'-(\", \":'(\"]\n",
    "happy_cry = [\":'-)\", \":')\"]\n",
    "horror = [\"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D-':\"]\n",
    "surprised = [\">:O\", \":-O\", \":O\", \":-o\", \":o\", \"8-0\", \"O_O\", \"o-o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\"]\n",
    "kiss= [\":*\", \":^*\", \"( '}{' )\"]\n",
    "wink = [\";-)\", \";)\", \"*-)\", \"*)\", \";-]\", \";]\", \";D\", \";^)\", \":-\"]\n",
    "tongue = [\">:P\", \":-P\", \":P\", \"X-P\", \"x-p\", \"xp\", \"XP\", \":-p\", \":p\", \"=p\", \":-b\", \":b\", \"d:\"]\n",
    "skeptical = [\">:\\ \".replace(\" \", \"\"), \">:/\", \":-/\", \":-.\", \":/\", \":\\ \".replace(\" \", \"\"), \"=/\", \"=\\ \".replace(\" \", \"\"), \":L\", \"=L\", \":S\", \">.<\"]\n",
    "neutral = [\":|\", \":-|\"]\n",
    "angel = [\"O:-)\", \"0:-3\", \"0:3\", \"0:-)\", \"0:)\", \"0;^)\"]\n",
    "evil = [\">:)\", \">;)\", \">:-)\", \"}:-)\", \"}:)\", \"3:-)\", \"3:)\"]\n",
    "high_five = [\"o/\\o\", \"^5\", \">_>^ ^<_<\"]\n",
    "heart = [\"<3\"]\n",
    "broken_hart = [\"</3\"]\n",
    "angry = [\":@\"]\n",
    "smiley_list = [ happy, laughing, sad, cry, happy_cry, horror, surprised, kiss, wink, tongue, skeptical, neutral, angel, evil, high_five, heart, broken_hart, angry]\n",
    "smiley_names = [ \"happy\", \"laughing\", \"sad\", \"cry\", \"happy_cry\", \"horror\", \"surprised\", \"kiss\", \"wink\", \"tongue\", \"skeptical\", \"neutral\", \"angel\", \"evil\", \"high_five\", \"heart\", \"broken_hart\",  \"angry\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def creation_smiley(reviews, smiley_list):\n",
    "    smiley_array = []\n",
    "    for smileys in smiley_list:\n",
    "        smiley_vect = [sum(review.count(sm) for sm in smileys) for review in reviews]\n",
    "        smiley_array.append(smiley_vect)\n",
    "    return np.array(smiley_array).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get anything being before \" /10\" or \"over ten\" or between both\n",
    "def get_all_mentionned_grade(reviews):\n",
    "    rev_grade = []\n",
    "    for idr,review in enumerate(reviews):\n",
    "        rev_grade.append([])\n",
    "        review_split= review.split()\n",
    "\n",
    "        for idw, word in enumerate(review_split):\n",
    "            try:\n",
    "                if((word==\"on\" or word==\"over\") and idw+1<len(review_split)):\n",
    "                    if(review_split[idw+1][0:3]==\"ten\" or review_split[idw+1][0:2]==\"10\"):\n",
    "                        if(idw>0):\n",
    "                             rev_grade[idr].append(review_split[idw-1])\n",
    "            except: print review_split\n",
    "\n",
    "            for idx, char in enumerate(word):\n",
    "                    if char == '/':\n",
    "                        ten_is_there= False\n",
    "                        if(idx < len(word)-2):\n",
    "                            if word[idx+1] == '1' and word[idx+2] == '0':\n",
    "                                ten_is_there=True\n",
    "                        if(idx < len(word)-3):\n",
    "                            if word[idx+1] == 't' and word[idx+2] == 'e' and word[idx+3]==\"n\":\n",
    "                                ten_is_there=True\n",
    "                        if(idx== len(word) -1 and idw<len(review_split)-1 and len(review_split[idw+1])>1 ):\n",
    "                            if((review_split[idw+1][0]=='1' and review_split[idw+1][1]=='0') or review_split[idw+1][0:3]==\"ten\"):\n",
    "                                ten_is_there=True\n",
    "\n",
    "                        if(ten_is_there):                  \n",
    "                            if(idx)>0:\n",
    "                                rev_grade[idr].append(word[0:idx])\n",
    "                            else:\n",
    "                                if(idw>0):\n",
    "                                    rev_grade[idr].append(review_split[idw-1])\n",
    "    return rev_grade\n",
    "\n",
    "\n",
    "# Convert what have been collected to a grade. If impossible, then the grade is 5.4321\n",
    "def convert_to_real_grade(grade):\n",
    "    new_grade = 5.4321\n",
    "    \n",
    "    ### The grade is a float\n",
    "    try:\n",
    "        new_grade = float(grade)\n",
    "        return float(new_grade)\n",
    "    \n",
    "    ### The grade is not a float\n",
    "    except:\n",
    "        good = '0123465789'\n",
    "        numerical_words = {'zero':0, 'one':1, 'two':1, 'three':3, 'four':4, 'five':5, \n",
    "                           'six':6, 'seven':7, 'height':8, 'nine':9, 'ten':10}\n",
    "        \n",
    "        ## The grade has numerical values at the end \n",
    "        if grade[-1] in good:\n",
    "\n",
    "            ### Read the grade in the string\n",
    "            one_dot = False\n",
    "            g_new = ''\n",
    "            for char in reversed(grade):\n",
    "                if char in good:\n",
    "                    g_new = char + g_new\n",
    "                elif char in '.,' and one_dot == False:\n",
    "                    g_new  = '.' + g_new\n",
    "                    one_dot = True\n",
    "                else:\n",
    "                    if g_new[0] in '.,':\n",
    "                        new_grade = g_new[1:]\n",
    "                    else:\n",
    "                        new_grade = g_new\n",
    "                    \n",
    "        elif (grade[-1] not in good) and (grade.lower() in numerical_words):\n",
    "            new_grade = numerical_words[grade.lower()]\n",
    "            \n",
    "        return float(new_grade)\n",
    "    \n",
    "\n",
    "# Same as previous but in the case that the reviews mentionned many grades\n",
    "def convert_to_real_grade_2(grade):\n",
    "    final_grade = 5.4321\n",
    "    new_grades = []\n",
    "    for i in range(len(grade)):\n",
    "        new_grade = convert_to_real_grade(grade[i])\n",
    "        new_grades.append(new_grade)\n",
    "        \n",
    "    if final_grade not in new_grades:\n",
    "        ## NEXT CONDITION IS TO BE DISCUSSED\n",
    "        if np.max(new_grades) - np.min(new_grades) < 7:\n",
    "            final_grade = np.mean(new_grades)\n",
    "    return final_grade\n",
    "\n",
    "\n",
    "# Return a single grade for each review\n",
    "def creation_grade(reviews):\n",
    "    rev_grade = get_all_mentionned_grade(reviews)\n",
    "    new_rev_grade = []\n",
    "    \n",
    "    for idg, grade in enumerate(rev_grade):    \n",
    "        converted_grade = 5.4321\n",
    "\n",
    "        if grade != []:\n",
    "            if len(grade) == 1:\n",
    "                converted_grade = convert_to_real_grade(grade[0])\n",
    "            else:\n",
    "                converted_grade = convert_to_real_grade_2(grade)\n",
    "        new_rev_grade.append(converted_grade)\n",
    "        \n",
    "    return new_rev_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return two OneHotEncoder (Or dummies)\n",
    "# The first says if it is a good movie\n",
    "# The second says if it is a bad movie\n",
    "def creation_good_grade_bad_grade(grades):\n",
    "    good_grade = []\n",
    "    bad_grade = []\n",
    "\n",
    "    for grade in grades:\n",
    "        if float(grade) > 6.8:\n",
    "            good_grade.append(True)\n",
    "            bad_grade.append(False)\n",
    "        elif float(grade) <4.0:\n",
    "            good_grade.append(False)\n",
    "            bad_grade.append(True)\n",
    "        else:\n",
    "            good_grade.append(False)\n",
    "            bad_grade.append(False)\n",
    "            \n",
    "    return np.array([good_grade, bad_grade]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imdbpie import Imdb\n",
    "\n",
    "def good_and_bad_movies():\n",
    "    good_movies = {}\n",
    "    bad_movies = {}\n",
    "    \n",
    "    imdb = Imdb(anonymize = True)\n",
    "    for movie in imdb.top_250():\n",
    "        good_movies[movie['title']] = movie['rating']\n",
    "        \n",
    "    return good_movies, bad_movies\n",
    "\n",
    "def mentionned_movies(reviews):\n",
    "    good_movies, bad_movies = good_and_bad_movies()\n",
    "    wrong_titles = wrong_titles = ['M', 'Up', 'Ran']\n",
    "    \n",
    "    rev_movies = []\n",
    "    \n",
    "    for idr,review in enumerate(reviews):   \n",
    "        # Movie in the review\n",
    "        movies = []\n",
    "        for key, value in good_movies.items():\n",
    "            if key in review and key not in wrong_titles:\n",
    "                movies.append(value)\n",
    "        for key, value in bad_movies.items():\n",
    "            if key in review and key not in wrong_titles:\n",
    "                movies.append(value)\n",
    "                \n",
    "        rev_movies.append(movies)\n",
    "    \n",
    "    return rev_movies\n",
    "    \n",
    "def creation_good_bad_mentionned_movies(reviews):\n",
    "    list_mentionned_movies = mentionned_movies(reviews)\n",
    "    \n",
    "    good_movie_mentionned = []\n",
    "    bad_movie_mentionned = []\n",
    "\n",
    "    for movie in list_mentionned_movies:\n",
    "        if movie > 6.8:\n",
    "            good_movie_mentionned.append(True)\n",
    "            bad_movie_mentionned.append(False)\n",
    "        elif movie < 4.0:\n",
    "            good_movie_mentionned.append(False)\n",
    "            bad_movie_mentionned.append(True)\n",
    "        else:\n",
    "            good_movie_mentionned.append(False)\n",
    "            bad_movie_mentionned.append(False)\n",
    "            \n",
    "    return np.array([good_movie_mentionned, bad_movie_mentionned]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the feature creation pipeline (depending on test or train):\n",
    "- Splitting train_train and test_train\n",
    "- Cleaning : HTML\n",
    "- Feature Creation : Excla, Inter, Length, WordCount, Susp, Smiley, Movie & Grade\n",
    "- Cleaning : Punctuation, Stopwords & Steemisation\n",
    "- TfIdf on the train only! \n",
    "- Transform the test according to the tfidf\n",
    "- Add the previously created features to the tfidf matrix\n",
    "- Store them\n",
    "\n",
    "### N.B. : At the end, no more splitting because the algorithm has to be learnt on all the data, and apply to the test: \n",
    "- train_train => train\n",
    "- train_test => test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_reviews, test_reviews, train_label, test_label = train_test_split(reviews, Class, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline(reviews, smiley_list):\n",
    "    \n",
    "    print 'Cleaning Lower, HTML, Abbreviation'\n",
    "    reviews = clean_lower(reviews)\n",
    "    reviews = clean_HTML(reviews)\n",
    "    reviews = clean_abbreviation(reviews)\n",
    "    \n",
    "    print 'Feature Creation : Excla, Inter, Length, Wordcount, Susp'\n",
    "    features = creation_excla_inter_length_wordcount_susp(reviews)\n",
    "    \n",
    "    print 'Feature Creation : Smileys'\n",
    "    smileys = creation_smiley(reviews, smiley_list)\n",
    "    \n",
    "    print 'Feature Creation : Good and bad movies mentionned'\n",
    "    movies = creation_good_bad_mentionned_movies(reviews)\n",
    "    \n",
    "    print 'Feature Creation : Good grades, bad grades'\n",
    "    several_grades = creation_grade(reviews)\n",
    "    grades = creation_good_grade_bad_grade(several_grades)\n",
    "    \n",
    "    print 'Cleaning Punctuation & Stopwords'\n",
    "    reviews = clean_punctuation(reviews)\n",
    "    reviews = clean_stopwords(reviews)\n",
    "\n",
    "    print 'Stemmization'\n",
    "    reviews = stemmisation(reviews)\n",
    "    \n",
    "    return reviews, np.concatenate([features, smileys, movies, grades], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259335ad54ac5fe35a61e25fe9\n",
      "['hahaha ;)', ':D :p'] LOL\n",
      "Cleaning Lower, HTML, Abbreviation\n",
      "Feature Creation : Excla, Inter, Length, Wordcount, Susp\n",
      "Feature Creation : Smileys\n",
      "Feature Creation : Good and bad movies mentionned\n",
      "Feature Creation : Good grades, bad grades\n",
      "Cleaning Punctuation & Stopwords\n",
      "Stemmization\n",
      "([u'hahaha', u'd p'], array([[0, 0, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0],\n",
      "       [0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "print'259335ad54ac5fe35a61e25fe9'\n",
    "test =  \"I really liked the movie, it's fucking awesome!!!! I looove it! Gonna Wanna go again! I'm You're<br />.they'll wouldn't clear not it's he's :) :() :( >:P 5/10 :D 8.61/10 BAAAAAAAAAD 100 000 dollars $$\"\n",
    "test2 = [\"hahaha ;)\", \":D :p\"]\n",
    "print test2, 'LOL'\n",
    "print pipeline(test2, smiley_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Lower, HTML, Abbreviation\n",
      "Feature Creation : Excla, Inter, Length, Wordcount, Susp\n",
      "Feature Creation : Smileys\n",
      "Feature Creation : Good and bad movies mentionned\n",
      "Feature Creation : Good grades, bad grades\n",
      "Cleaning Punctuation & Stopwords\n",
      "Stemmization\n",
      "Cleaning Lower, HTML, Abbreviation\n",
      "Feature Creation : Excla, Inter, Length, Wordcount, Susp\n",
      "Feature Creation : Smileys\n",
      "Feature Creation : Good and bad movies mentionned\n",
      "Feature Creation : Good grades, bad grades\n",
      "Cleaning Punctuation & Stopwords\n",
      "Stemmization\n",
      "Cleaning Lower, HTML, Abbreviation\n",
      "Feature Creation : Excla, Inter, Length, Wordcount, Susp\n",
      "Feature Creation : Smileys\n",
      "Feature Creation : Good and bad movies mentionned\n",
      "Feature Creation : Good grades, bad grades\n",
      "Cleaning Punctuation & Stopwords\n",
      "Stemmization\n"
     ]
    }
   ],
   "source": [
    "train_reviews, train_feat = pipeline(train_reviews,smiley_list)\n",
    "test_reviews, test_feat = pipeline(test_reviews, smiley_list)\n",
    "test_test_reviews, test_test_feat = pipeline(test_test_reviews, smiley_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf matrix\n",
    "- Fit and transform on the train_reviews\n",
    "- Transform the test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transform and git on the training reviews\n",
    "#TO BE DISCUSSED : parameters : max_df, min_df, max_features\n",
    "ngram_range = (1,4)        # trigrams\n",
    "min_df = 5./25000 # considers tTfidfVectorizererms that appears in more than 5 documents\n",
    "max_features = 200000    #considers only the top 10000 mono- and bi- grams ordered by term frequency across the corpus\n",
    "m = TfidfVectorizer(ngram_range = ngram_range, min_df = min_df, max_features = max_features, sublinear_tf = True)\n",
    "\n",
    "# Train the model with all the available data\n",
    "data = np.concatenate([train_reviews, test_reviews, test_test_reviews])\n",
    "train_tfidf = m.fit_transform(data)\n",
    "\n",
    "# Keep all the data in data & extract the first 18750 rows that are the real train_reviews\n",
    "data = train_tfidf\n",
    "train_tfidf = train_tfidf[:18750]\n",
    "\n",
    "# Transform the test_reviews\n",
    "test_tfidf = m.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data : \n",
    "- Concatenate the tfidf matrices and the new features\n",
    "- Get the name of the features\n",
    "- Storing the two matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b): #b est un vecteur ligne (np.array ou liste) et a est une sparse matrix\n",
    "    if(type(a)!= scipy.sparse.csr.csr_matrix):\n",
    "        a=scipy.sparse.csr_matrix(a)\n",
    "        \n",
    "    if(type(b)== list):\n",
    "        b=np.array([b]).T\n",
    "    if(type(b)!= scipy.sparse.csr.csr_matrix):\n",
    "        b=scipy.sparse.csr_matrix(b)\n",
    "        \n",
    "    return scipy.sparse.hstack([a,b], format ='csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate for the train matrix\n",
    "train_tfidf = csr_vappend(train_tfidf, train_feat)\n",
    "test_tfidf = csr_vappend(test_tfidf, test_feat)\n",
    "\n",
    "# Name of the features\n",
    "features = m.get_feature_names()\n",
    "features.append('excla')\n",
    "features.append('inter')\n",
    "features.append('length')\n",
    "features.append('wordcount')\n",
    "features.append('susp')\n",
    "for smiley in smiley_names:\n",
    "    features.append(smiley)\n",
    "features.append('good_movie_mentionned')\n",
    "features.append('bad_movie_mentionned')\n",
    "features.append('good_grade')\n",
    "features.append('bad_grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [row[0] for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train_4', train_tfidf)\n",
    "save_sparse_csr('data_test_4', test_tfidf)\n",
    "save_csv('label_train.csv', train_label)\n",
    "save_csv('label_test.csv', test_label)\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    features[i] = feat.encode('utf8', 'replace')\n",
    "save_csv('feature_names.csv', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18750, 90208) (6250, 90208)\n"
     ]
    }
   ],
   "source": [
    "print train_tfidf.shape, test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Things to do :\n",
    "- For the Tf-Idf, separate the sentences before training so that \"You love. I hate\" doesn't give the 2-gram \"love I\"\n",
    "- Concatenate \"100 000\" to \"100000\" (There exists a non negligeable amout of \"x 000\")\n",
    "- Erase the smileys when found because \":D\" returns \"d\". Maybe the same for grades \"0/10\" return \"0 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
