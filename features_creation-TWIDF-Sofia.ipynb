{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith open(rootdir+\"/\"+file, 'r', endoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_sofia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data cleaning:\n",
    "- Remove all the HTML symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove HLML signs\n",
    "HTMLlist = ['<br />']\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    for word in HTMLlist:\n",
    "        reviews[idx] = review.replace(word,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature creation:\n",
    "- List punctuation (various form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla = [0]*len(reviews)\n",
    "inter = [0]*len(reviews)\n",
    "susp = [0]*len(reviews)\n",
    "for i, review in enumerate(reviews):\n",
    "    for char in review:\n",
    "        if char == \"?\":\n",
    "            inter[i] += 1\n",
    "        elif char == \"!\":\n",
    "            excla[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "ttoken = TweetTokenizer(reduce_len=True)\n",
    "tokenized_reviews = []\n",
    "\n",
    "for review in reviews:\n",
    "    tokenized_reviews.append(ttoken.tokenize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, review in enumerate(tokenized_reviews):\n",
    "    for word in review:\n",
    "        if word == \"...\":\n",
    "            susp[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature creation : \n",
    "- Length of the review\n",
    "- Movie mentionned in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_length = []\n",
    "rev_word_count = []\n",
    "\n",
    "\n",
    "for idr,review in enumerate(reviews):\n",
    "    # Length of the review\n",
    "    rev_length.append(len(review))\n",
    "    \n",
    "    rev_word_count.append(0)\n",
    "    for word in review:\n",
    "        rev_word_count[idr]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The movie list has to be completed\n",
    "# - Find a list of the movie so as it matches some in the reviews : www.imdb.com\n",
    "\n",
    "rev_movie = []\n",
    "\n",
    "movie_list = {'Titanic':9}\n",
    "for idr,review in enumerate(reviews):   \n",
    "    # Movie in the review\n",
    "    movies = []\n",
    "    for key, value in movie_list.items():\n",
    "        if key in review:\n",
    "            movies.append(value)\n",
    "    rev_movie.append(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rev_movie = []\n",
    "\n",
    "for movies in rev_movie:\n",
    "    # No movies quoted\n",
    "    if movies == []:\n",
    "        new_rev_movie.append(-1000)\n",
    "    else:\n",
    "        # Only one movie quoted\n",
    "        if len(movies) == 1:\n",
    "            new_rev_movie.append(movies[0])\n",
    "        # Different movies quoted - TO BE UPGRADED\n",
    "        else:\n",
    "            new_rev_movie.append(-1000)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation :\n",
    "- Grade mentionned in the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_grade = []\n",
    "# Grade/Mark in the review\n",
    "for idr,review in enumerate(reviews):\n",
    "    rev_grade.append([])\n",
    "    review_split= review.split(\" \")\n",
    "\n",
    "    for idw, word in enumerate(review_split):\n",
    "        for idx, char in enumerate(word):\n",
    "                if char == '/':\n",
    "                    ten_is_there= False\n",
    "                    if(idx < len(word)-2):\n",
    "                        if word[idx+1] == '1' and word[idx+2] == '0':\n",
    "                            ten_is_there=True\n",
    "                    if(idx== len(word) -1 and idw<len(review_split)-1 and len(review_split[idw+1])>1 ):\n",
    "                        if(review_split[idw+1][0]=='1' and review_split[idw+1][1]=='0'):\n",
    "                            ten_is_there=True\n",
    "                    if(ten_is_there):                  \n",
    "                        if(idx)>0:\n",
    "                            rev_grade[idr].append(word[0:idx])\n",
    "                        else:\n",
    "                            rev_grade[idr].append(review_split[idw-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for reviews with one grade\n",
    "\n",
    "# DO NOT DELETE THE PRINT NOW : The function may be better when used on the test set\n",
    "\n",
    "# SOMETIMES : 1/10/2015 -> It is a date ! \n",
    "\n",
    "def convert_to_real_grade(grade):\n",
    "    new_grade = -100\n",
    "    \n",
    "    ### The grade is a float\n",
    "    try:\n",
    "        new_grade = float(grade)\n",
    "        return float(new_grade)\n",
    "    \n",
    "    ### The grade is not a float\n",
    "    except:\n",
    "        good = '0123465789'\n",
    "        numerical_words = {'zero':0, 'one':1, 'two':1, 'three':3, 'four':4, 'five':5, \n",
    "                           'six':6, 'seven':7, 'height':8, 'nine':9, 'ten':10}\n",
    "        \n",
    "        ## The grade has numerical values at the end \n",
    "        if grade[-1] in good:\n",
    "\n",
    "            ### Read the grade in the string\n",
    "            one_dot = False\n",
    "            g_new = ''\n",
    "            for char in reversed(grade):\n",
    "                if char in good:\n",
    "                    g_new = char + g_new\n",
    "                elif char in '.,' and one_dot == False:\n",
    "                    g_new  = '.' + g_new\n",
    "                    one_dot = True\n",
    "                else:\n",
    "                    if g_new[0] in '.,':\n",
    "                        new_grade = g_new[1:]\n",
    "                    else:\n",
    "                        new_grade = g_new\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        elif (grade[-1] not in good) and (grade.lower() in numerical_words):\n",
    "            new_grade = numerical_words[grade.lower()]\n",
    "            \n",
    "        return float(new_grade)\n",
    "                \n",
    "                    \n",
    "\n",
    "# functions for reviews with more than one grade\n",
    "def convert_to_real_grade_2(grade):\n",
    "    new_grades = []\n",
    "    for i in range(len(grade)):\n",
    "        new_grade = convert_to_real_grade(grade[i])\n",
    "        new_grades.append(new_grade)\n",
    "    #return new_grades\n",
    "    return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_rev_grade = []\n",
    "for idg, grade in enumerate(rev_grade):    \n",
    "    converted_grade = -1000\n",
    "    \n",
    "    if grade != []:\n",
    "        if len(grade) == 1:\n",
    "            converted_grade = convert_to_real_grade(grade[0])\n",
    "        else:\n",
    "            converted_grade = convert_to_real_grade_2(grade)\n",
    "    new_rev_grade.append(converted_grade)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features : Other ideas to try\n",
    "\n",
    "- Find N-grams where it may start with a CAPITAL (As for the movie names and actor's names)\n",
    "- Add smileys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "happy = [\":-)\", \":)\", \":D\", \":o)\", \":]\", \":3\", \":c)\", \":>\", \"=]\", \"8)\", \"=)\", \":}\", \":^)\", \":-))\", \"^^\"]\n",
    "laughing = [\":-D\", \"8-D\", \"8D\", \"x-D\", \"xD\", \"X-D\", \"XD\", \"=-D\", \"=D\", \"=-3\", \"=3\", \"B^D\"]\n",
    "sad = [\">:[\", \":-(\", \":(\", \":-c\", \":c\", \":-<\", \":<\", \":-[\", \":[\", \":{\", \";(\"]\n",
    "cry = [\":'-(\", \":'(\"]\n",
    "happy_cry = [\":'-)\", \":')\"]\n",
    "horror = [\"D:<\", \"D:\", \"D8\", \"D;\", \"D=\", \"DX\", \"v.v\", \"D-':\"]\n",
    "surprised = [\">:O\", \":-O\", \":O\", \":-o\", \":o\", \"8-0\", \"O_O\", \"o-o\", \"O_o\", \"o_O\", \"o_o\", \"O-O\"]\n",
    "kiss= [\":*\", \":^*\", \"( '}{' )\"]\n",
    "wink = [\";-)\", \";)\", \"*-)\", \"*)\", \";-]\", \";]\", \";D\", \";^)\", \":-\"]\n",
    "tongue = [\">:P\", \":-P\", \":P\", \"X-P\", \"x-p\", \"xp\", \"XP\", \":-p\", \":p\", \"=p\", \":-b\", \":b\", \"d:\"]\n",
    "skeptical = [\">:\\ \".replace(\" \", \"\"), \">:/\", \":-/\", \":-.\", \":/\", \":\\ \".replace(\" \", \"\"), \"=/\", \"=\\ \".replace(\" \", \"\"), \":L\", \"=L\", \":S\", \">.<\"]\n",
    "neutral = [\":|\", \":-|\"]\n",
    "angel = [\"O:-)\", \"0:-3\", \"0:3\", \"0:-)\", \"0:)\", \"0;^)\"]\n",
    "evil = [\">:)\", \">;)\", \">:-)\", \"}:-)\", \"}:)\", \"3:-)\", \"3:)\"]\n",
    "high_five = [\"o/\\o\", \"^5\", \">_>^ ^<_<\"]\n",
    "heart = [\"<3\"]\n",
    "broken_hart = [\"</3\"]\n",
    "angry = [\":@\"]\n",
    "smiley_list = [\n",
    "happy,\n",
    "laughing,\n",
    "sad,\n",
    "cry,\n",
    "happy_cry,\n",
    "horror,\n",
    "surprised,\n",
    "kiss,\n",
    "wink,\n",
    "tongue,\n",
    "skeptical,\n",
    "neutral,\n",
    "angel,\n",
    "evil,\n",
    "high_five,\n",
    "heart,\n",
    "broken_hart, \n",
    "angry]\n",
    "smiley_names = [\n",
    "\"happy\",\n",
    "\"laughing\",\n",
    "\"sad\",\n",
    "\"cry\",\n",
    "\"happy_cry\",\n",
    "\"horror\",\n",
    "\"surprised\",\n",
    "\"kiss\",\n",
    "\"wink\",\n",
    "\"tongue\",\n",
    "\"skeptical\",\n",
    "\"neutral\",\n",
    "\"angel\",\n",
    "\"evil\",\n",
    "\"high_five\",\n",
    "\"heart\",\n",
    "\"broken_hart\", \n",
    "\"angry\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_features_smiley(tokenized_text, smiley_list):\n",
    "    features_smiley = np.zeros((len(tokenized_text),len(smiley_list)))\n",
    "    for i, review in enumerate(tokenized_text):\n",
    "        for w in review :\n",
    "            if len(w)<2 : \n",
    "                pass\n",
    "            elif len(w)>5:\n",
    "                pass\n",
    "            for j, cat in enumerate(smiley_list):\n",
    "                if w in cat:\n",
    "                    features_smiley[i,j] = 1\n",
    "    return features_smiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_smiley = gen_features_smiley(tokenized_reviews, smiley_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Data Cleaning (After the features creation) :\n",
    " - Punctuation\n",
    " - Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove punctuation, lower all characters\n",
    "# exclude = {',' ,'+', '<', ':', '/', ']', '(', ')', '{', '\"', '_', '?', '@', '}', ...}\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w if w not in exclude else \" \" for w in review.lower() ])\n",
    "    \n",
    "# Remove stop words based on the given list - To be changed depending on the needs\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([w +' ' for w in review.split() if w not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Data Cleaning: \n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steeming -> Reduce words to their initial mining\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([stemmer.stem(word)+' ' for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - Idf Matrix\n",
    "\n",
    "#### To be upgraded with new tf and idf functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Size of the tfidf matrix: ', 3445861)\n",
      "(25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "# Features extraction with TF - IDF : get the matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "m = TfidfVectorizer()\n",
    "tfidf_matrix = m.fit_transform(reviews)\n",
    "\n",
    "print(\"Size of the tfidf matrix: \", tfidf_matrix.size)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words=list(set(''.join([review for review in reviews ]).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_documents = len(reviews)\n",
    "sliding_window = 2\n",
    "train_par = True\n",
    "idf_learned = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createGraphFeatures(num_documents,clean_train_documents,unique_words,sliding_window,train_par,idf_learned):\n",
    "\tfeatures = np.zeros((num_documents,len(unique_words)))#where we are going to put the features\n",
    "\tunique_words_len = len(unique_words)\n",
    "\tterm_num_docs = {} #dictionay of each word with a count of that word through out the collections\n",
    "\tidf_col = {}#dictionay of each word with the idf of that word\n",
    "\t\n",
    "\t#TO DO:\n",
    "\t#1.idf_col:IDF for the collection\n",
    "\t#\tif in training phase compute it\n",
    "\t#\telse use the one provided\n",
    "\t#2. term_num_docs : count of the words in the collection\n",
    "\t#\tif in training phase populate it\n",
    "\t#\telse use the one provided\n",
    "\tif train_par:\n",
    "\t\t\n",
    "\t\t#for all documents\n",
    "\t\tfor i in range( 0,num_documents ):\n",
    "\t\t\tif i%100 == 0 :\n",
    "\t\t\t\tprint \"idf\", i\n",
    "\t\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\t\t#count word occurrences through the collection (for idf) put the count in term_num_docs\n",
    "\t\t\tif len(wordList)>1:\n",
    "\t\t\t\tcountWords(wordList,term_num_docs) #TODO: implement this function \n",
    "\t\t#TODO: calculate the idf for all words\n",
    "\t\tfor term_x in term_num_docs:\n",
    "\t\t\tidf_col[term_x] = np.log10(float(num_documents)/term_num_docs[term_x])            \n",
    "\t# for the testing set\n",
    "\telse:\n",
    "\t\t#use the existing ones if we are in the test data\n",
    "\t\tidf_col = idf_learned \n",
    "\t\tterm_num_docs=unique_words\n",
    "\n",
    "\tprint \"Creating the graph of words for each document...\"\n",
    "\ttotalNodes = 0\n",
    "\ttotalEdges = 0\n",
    "\n",
    "\t#go over all documents\n",
    "\tfor i in range( 0,num_documents ):\n",
    "\t\tif i%100 == 0 :\n",
    "\t\t\tprint \"tw\", i\n",
    "\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\tdocLen = len(wordList)\n",
    "\t\t#the graph\n",
    "\t\tdG = nx.Graph()\n",
    "\n",
    "\t\tif len(wordList)>1:\n",
    "\t\t\tpopulateGraph(wordList,dG,sliding_window)\n",
    "\t\t\tdG.remove_edges_from(dG.selfloop_edges())\n",
    "\t\t\tcentrality = nx.degree_centrality(dG) #dictionary of centralities (node:degree)\n",
    "\n",
    "\t\t\ttotalNodes += dG.number_of_nodes()\n",
    "\t\t\ttotalEdges += dG.number_of_edges()\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t#TODO : implement comments bellow\n",
    "\t\t\t# for all nodes\n",
    "\t\t\t\t#If they are in the desired features\n",
    "\t\t\t\t\t#compute the TW-IDF score and put it in features[i,unique_words.index(g)]\n",
    "\t\t\tfor k, node_term in enumerate(dG.nodes()):\n",
    "\t\t\t\tif node_term in idf_col:\n",
    "\t\t\t\t\tfeatures[i,unique_words.index(node_term)] = centrality[node_term] * idf_col[node_term]\n",
    "\n",
    "\tif train_par:\n",
    "\t\tnodes_ret=term_num_docs.keys()\n",
    "\t\t#print \"Percentage of features kept:\"+str(feature_reduction)\n",
    "\t\tprint \"Average number of nodes:\"+str(float(totalNodes)/num_documents)\n",
    "\t\tprint \"Average number of edges:\"+str(float(totalEdges)/num_documents)\n",
    "\telse:\n",
    "\t\tnodes_ret=term_num_docs\n",
    "\t#return 1: features, 2: idf values (for the test data), 3: the list of terms \n",
    "\treturn features, idf_col, nodes_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populateGraph(wordList,dG,sliding_window):\n",
    "\t#TODO: implement this function\n",
    "\t#For each position/word in the word list:\n",
    "\t\t#add the -new- word in the graph\n",
    "\t\t#for all words -forward- within the window size\n",
    "\t\t\t#add new words as new nodes \n",
    "\t\t\t#add edges among all word within the window\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif not dG.has_node(word):\n",
    "\t\t\tdG.add_node(word)\n",
    "\t\ttempW=sliding_window\n",
    "\t\tif k+sliding_window > len(wordList):\n",
    "\t\t\ttempW=len(wordList) - k\n",
    "\t\tfor j in xrange(1, tempW):\n",
    "\t\t\tnext_word = wordList[k+j]\n",
    "\t\t\tdG.add_edge(word, next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords(wordList,term_num_docs):\n",
    "\tfound = set()\n",
    "\t#TODO: implement this function\n",
    "\t#add the terms from the wordlist to the term_num_docs dictionary or increase its count\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif word not in found:\n",
    "\t\t\tfound.add(word)\n",
    "\t\t\tif word in term_num_docs:\n",
    "\t\t\t\tterm_num_docs[word] +=1\n",
    "\t\t\telse :\n",
    "\t\t\t\tterm_num_docs[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw_idf=createGraphFeatures(num_documents,reviews,unique_words,sliding_window,train_par,idf_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(tw_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(pd.DataFrame(tw_idf[0]), path=\"/Users/Flukmacdesof/projet-MAP-670\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new features to the Tf-Idf matrix\n",
    "#### New features are : \n",
    "- Number of exclamation point\n",
    "- Number of interrogation point\n",
    "- Number of suspension point\n",
    "- Review length\n",
    "- Number of word (word_count)\n",
    "- Movie mentionned\n",
    "- Grade mentionned\n",
    "\n",
    "- Smileys (How to deal with them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b):\n",
    "    \"\"\" Takes in 2 csr_matrices and appends the second one to the bottom of the first one. \n",
    "    Much faster than scipy.sparse.vstack but assumes the type to be csr and overwrites\n",
    "    the first matrix instead of copying it. The data, indices, and indptr still get copied.\"\"\"\n",
    "\n",
    "    a.data = np.hstack((a.data,b.data))\n",
    "    a.indices = np.hstack((a.indices,b.indices))\n",
    "    a.indptr = np.hstack((a.indptr,(b.indptr + a.nnz)[1:]))\n",
    "    a._shape = (a.shape[0]+b.shape[0],b.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla = scipy.sparse.csr_matrix(excla)\n",
    "inter = scipy.sparse.csr_matrix(inter)\n",
    "susp = scipy.sparse.csr_matrix(susp)\n",
    "rev_length = scipy.sparse.csr_matrix(rev_length)\n",
    "rev_word_count = scipy.sparse.csr_matrix(rev_word_count)\n",
    "rev_grade = scipy.sparse.csr_matrix(new_rev_grade)\n",
    "rev_movie = scipy.sparse.csr_matrix(new_rev_movie)\n",
    "rev_smiley = scipy.sparse.csr_matrix(features_smiley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csr_vappend(tfidf_matrix, excla)\n",
    "csr_vappend(tfidf_matrix, inter)\n",
    "csr_vappend(tfidf_matrix, susp)\n",
    "csr_vappend(tfidf_matrix,rev_length)\n",
    "csr_vappend(tfidf_matrix,rev_word_count)\n",
    "csr_vappend(tfidf_matrix,rev_grade)\n",
    "csr_vappend(tfidf_matrix,rev_movie)\n",
    "csr_vappend(tfidf_matrix, rev_smiley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CSV Creation:\n",
    "\n",
    "Create 5 csv : train_train.csv, train_test.csv, y_train_train.csv, y_train_test.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tf-idf matrix into two data sets to process the cross validation : training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(tfidf_matrix, Class, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tfidf_matrix.shape\n",
    "Class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [float(row[0]) for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train', data_train)\n",
    "save_sparse_csr('data_test', data_test)\n",
    "save_csv('label_train.csv', label_train)\n",
    "save_csv('label_test.csv', label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
