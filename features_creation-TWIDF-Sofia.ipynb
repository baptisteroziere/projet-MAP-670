{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "path_sofia_server = \"/home/sofia.calcagno/data 2/train\"\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith open(rootdir+\"/\"+file, 'r', endoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_sofia_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_sample = np.concatenate([reviews[:100], reviews[-100:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Class_sample = np.concatenate([Class[:100], Class[-100:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def data_preparation_twidf(reviews):\n",
    "    HTMLlist = ['<br />'] # list of HLML signs\n",
    "    exclude = set(string.punctuation) #list of punctuation \n",
    "    swords = stopwords.words(\"english\") #list of stopwords\n",
    "    for idx, review in enumerate(reviews):\n",
    "        for word in HTMLlist:\n",
    "            reviews[idx] = review.replace(word,' ') # Remove HLML signs\n",
    "        reviews[idx] = ''.join([stemmer.stem(word)+' ' for word in reviews[idx].split()]) #stemming\n",
    "        reviews[idx] = ''.join([w if w not in exclude else \" \" for w in reviews[idx].lower() ])\n",
    "        reviews[idx] = ''.join([w +' ' for w in reviews[idx].split() if w not in swords])\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_reviews = data_preparation_twidf(reviews)\n",
    "clean_sample_reviews = data_preparation_twidf(reviews_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words=list(set(''.join([review for review in clean_reviews ]).split(' ')))\n",
    "unique_words_sample=list(set(''.join([review for review in clean_sample_reviews ]).split(' ')))\n",
    "num_documents = len(reviews)\n",
    "num_documents_sample = len(reviews_sample)\n",
    "sliding_window = 2\n",
    "train_par = True\n",
    "idf_learned = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def createGraphFeatures(num_documents,clean_train_documents,unique_words,sliding_window,train_par,idf_learned):\n",
    "\tfeatures = np.zeros((num_documents,len(unique_words)))#where we are going to put the features\n",
    "\tunique_words_len = len(unique_words)\n",
    "\tterm_num_docs = {} #dictionay of each word with a count of that word through out the collections\n",
    "\tidf_col = {}#dictionay of each word with the idf of that word\n",
    "\t\n",
    "\t#TO DO:\n",
    "\t#1.idf_col:IDF for the collection\n",
    "\t#\tif in training phase compute it\n",
    "\t#\telse use the one provided\n",
    "\t#2. term_num_docs : count of the words in the collection\n",
    "\t#\tif in training phase populate it\n",
    "\t#\telse use the one provided\n",
    "\tif train_par:\n",
    "\t\t\n",
    "\t\t#for all documents\n",
    "\t\tfor i in range( 0,num_documents ):\n",
    "\t\t\tif i%100 == 0 :\n",
    "\t\t\t\tprint \"idf\", i\n",
    "\t\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\t\t#count word occurrences through the collection (for idf) put the count in term_num_docs\n",
    "\t\t\tif len(wordList)>1:\n",
    "\t\t\t\tcountWords(wordList,term_num_docs) #TODO: implement this function \n",
    "\t\t#TODO: calculate the idf for all words\n",
    "\t\tfor term_x in term_num_docs:\n",
    "\t\t\tidf_col[term_x] = np.log10(float(num_documents)/term_num_docs[term_x])            \n",
    "\t# for the testing set\n",
    "\telse:\n",
    "\t\t#use the existing ones if we are in the test data\n",
    "\t\tidf_col = idf_learned \n",
    "\t\tterm_num_docs=unique_words\n",
    "\n",
    "\tprint \"Creating the graph of words for each document...\"\n",
    "\ttotalNodes = 0\n",
    "\ttotalEdges = 0\n",
    "\n",
    "\t#go over all documents\n",
    "\tfor i in range( 0,num_documents ):\n",
    "\t\tif i%100 == 0 :\n",
    "\t\t\tprint \"tw\", i\n",
    "\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\tdocLen = len(wordList)\n",
    "\t\t#the graph\n",
    "\t\tdG = nx.Graph()\n",
    "\n",
    "\t\tif len(wordList)>1:\n",
    "\t\t\tpopulateGraph(wordList,dG,sliding_window)\n",
    "\t\t\tdG.remove_edges_from(dG.selfloop_edges())\n",
    "\t\t\tcentrality = nx.degree_centrality(dG) #dictionary of centralities (node:degree)\n",
    "\n",
    "\t\t\ttotalNodes += dG.number_of_nodes()\n",
    "\t\t\ttotalEdges += dG.number_of_edges()\n",
    "\t\t\t#TODO : implement comments bellow\n",
    "\t\t\t# for all nodes\n",
    "\t\t\t\t#If they are in the desired features\n",
    "\t\t\t\t\t#compute the TW-IDF score and put it in features[i,unique_words.index(g)]\n",
    "\t\t\tfor k, node_term in enumerate(dG.nodes()):\n",
    "\t\t\t\tif node_term in idf_col:\n",
    "\t\t\t\t\tfeatures[i,unique_words.index(node_term)] = centrality[node_term] * idf_col[node_term]\n",
    "\n",
    "\tif train_par:\n",
    "\t\tnodes_ret=term_num_docs.keys()\n",
    "\t\t#print \"Percentage of features kept:\"+str(feature_reduction)\n",
    "\t\tprint \"Average number of nodes:\"+str(float(totalNodes)/num_documents)\n",
    "\t\tprint \"Average number of edges:\"+str(float(totalEdges)/num_documents)\n",
    "\telse:\n",
    "\t\tnodes_ret=term_num_docs\n",
    "\t#return 1: features, 2: idf values (for the test data), 3: the list of terms \n",
    "\treturn features, idf_col, nodes_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populateGraph(wordList,dG,sliding_window):\n",
    "\t#TODO: implement this function\n",
    "\t#For each position/word in the word list:\n",
    "\t\t#add the -new- word in the graph\n",
    "\t\t#for all words -forward- within the window size\n",
    "\t\t\t#add new words as new nodes \n",
    "\t\t\t#add edges among all word within the window\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif not dG.has_node(word):\n",
    "\t\t\tdG.add_node(word)\n",
    "\t\tif k+sliding_window > len(wordList):\n",
    "\t\t\ttempW=len(wordList) - k\n",
    "\t\telse:\n",
    "\t\t\ttempW=sliding_window\n",
    "\t\tfor j in xrange(1, tempW):\n",
    "\t\t\tnext_word = wordList[k+j]\n",
    "\t\t\tdG.add_edge(word, next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords(wordList,term_num_docs):\n",
    "\tfound = set()\n",
    "\t#TODO: implement this function\n",
    "\t#add the terms from the wordlist to the term_num_docs dictionary or increase its count\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif word not in found:\n",
    "\t\t\tfound.add(word)\n",
    "\t\t\tif word in term_num_docs:\n",
    "\t\t\t\tterm_num_docs[word] +=1\n",
    "\t\t\telse :\n",
    "\t\t\t\tterm_num_docs[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf 0\n",
      "idf 100\n",
      "Creating the graph of words for each document...\n",
      "tw 0\n",
      "tw 100\n",
      "Average number of nodes:96.905\n",
      "Average number of edges:116.435\n"
     ]
    }
   ],
   "source": [
    "tw_idf=createGraphFeatures(num_documents_sample,clean_sample_reviews,unique_words_sample,sliding_window,train_par,idf_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_tw_idf = scipy.sparse.csr_matrix(tw_idf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CSV Creation:\n",
    "\n",
    "Create 5 csv : train_train.csv, train_test.csv, y_train_train.csv, y_train_test.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tf-idf matrix into two data sets to process the cross validation : training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(sparse_tw_idf, Class_sample, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [float(row[0]) for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_tw_train', data_train)\n",
    "save_sparse_csr('data_tw_test', data_test)\n",
    "save_csv('label_tw_train.csv', label_train)\n",
    "save_csv('label_tw_test.csv', label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
