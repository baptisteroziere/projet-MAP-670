{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "path_baptiste = \"/home/baptiste/Documents/data/train\"\n",
    "path_igor = \"C:/Users/Igor/Documents/Master Data Science/Big Data Analytics/Projet/Data/train\"\n",
    "path_sofia = \"/Users/Flukmacdesof/data 2/train\"\n",
    "\n",
    "\n",
    "\n",
    "#assumes labelled data ra stored into a positive and negative folder\n",
    "#returns two lists one with the text per file and another with the corresponding class \n",
    "def loadLabeled(path):\n",
    "\n",
    "\trootdirPOS =path+'/pos'\n",
    "\trootdirNEG =path+'/neg'\n",
    "\tdata=[]\n",
    "\tClass=[]\n",
    "\tcount=0\n",
    "\tfor subdir, dirs, files in os.walk(rootdirPOS):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirPOS+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc1=np.ones(len(data))\n",
    "\tfor subdir, dirs, files in os.walk(rootdirNEG):\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\twith codecs.open(rootdirNEG+\"/\"+file, 'r',encoding=\"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\ttmpc0=np.zeros(len(data)-len(tmpc1))\n",
    "\tClass=np.concatenate((tmpc1,tmpc0),axis=0)\n",
    "\treturn data,Class\n",
    "#loads unlabelled data\t\n",
    "#returns two lists\n",
    "#one with the data per file and another with the respective filenames (without the file extension)\n",
    "def loadUknown(path):\n",
    "\trootdir=path\n",
    "\tdata=[]\n",
    "\tnames=[]\n",
    "\tfor subdir, dirs, files in os.walk(rootdir):\n",
    "\t\tfor file in files:\n",
    "\t\t\twith open(rootdir+\"/\"+file, 'r', endoding= \"utf-8\") as content_file:\n",
    "\t\t\t\tcontent = content_file.read() #assume that there are NO \"new line characters\"\n",
    "\t\t\t\tdata.append(content)\n",
    "\t\t\t\tnames.append(file.split(\".\")[0])\n",
    "\treturn data,names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews, Class = loadLabeled(path_sofia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data cleaning:\n",
    "- Remove all the HTML symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove HLML signs\n",
    "HTMLlist = ['<br />']\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    for word in HTMLlist:\n",
    "        reviews[idx] = review.replace(word,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Data Cleaning: \n",
    "- Stemmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steeming -> Reduce words to their initial mining\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    reviews[idx] = ''.join([stemmer.stem(word)+' ' for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf - Idf Matrix\n",
    "\n",
    "#### To be upgraded with new tf and idf functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Size of the tfidf matrix: ', 3445861)\n",
      "(25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "# Features extraction with TF - IDF : get the matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "m = TfidfVectorizer()\n",
    "tfidf_matrix = m.fit_transform(reviews)\n",
    "\n",
    "print(\"Size of the tfidf matrix: \", tfidf_matrix.size)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words=list(set(''.join([review for review in reviews ]).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_documents = len(reviews)\n",
    "sliding_window = 2\n",
    "train_par = True\n",
    "idf_learned = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createGraphFeatures(num_documents,clean_train_documents,unique_words,sliding_window,train_par,idf_learned):\n",
    "\tfeatures = np.zeros((num_documents,len(unique_words)))#where we are going to put the features\n",
    "\tunique_words_len = len(unique_words)\n",
    "\tterm_num_docs = {} #dictionay of each word with a count of that word through out the collections\n",
    "\tidf_col = {}#dictionay of each word with the idf of that word\n",
    "\t\n",
    "\t#TO DO:\n",
    "\t#1.idf_col:IDF for the collection\n",
    "\t#\tif in training phase compute it\n",
    "\t#\telse use the one provided\n",
    "\t#2. term_num_docs : count of the words in the collection\n",
    "\t#\tif in training phase populate it\n",
    "\t#\telse use the one provided\n",
    "\tif train_par:\n",
    "\t\t\n",
    "\t\t#for all documents\n",
    "\t\tfor i in range( 0,num_documents ):\n",
    "\t\t\tif i%100 == 0 :\n",
    "\t\t\t\tprint \"idf\", i\n",
    "\t\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\t\t#count word occurrences through the collection (for idf) put the count in term_num_docs\n",
    "\t\t\tif len(wordList)>1:\n",
    "\t\t\t\tcountWords(wordList,term_num_docs) #TODO: implement this function \n",
    "\t\t#TODO: calculate the idf for all words\n",
    "\t\tfor term_x in term_num_docs:\n",
    "\t\t\tidf_col[term_x] = np.log10(float(num_documents)/term_num_docs[term_x])            \n",
    "\t# for the testing set\n",
    "\telse:\n",
    "\t\t#use the existing ones if we are in the test data\n",
    "\t\tidf_col = idf_learned \n",
    "\t\tterm_num_docs=unique_words\n",
    "\n",
    "\tprint \"Creating the graph of words for each document...\"\n",
    "\ttotalNodes = 0\n",
    "\ttotalEdges = 0\n",
    "\n",
    "\t#go over all documents\n",
    "\tfor i in range( 0,num_documents ):\n",
    "\t\tif i%100 == 0 :\n",
    "\t\t\tprint \"tw\", i\n",
    "\t\twordList = clean_train_documents[i].split(None)\n",
    "\t\tdocLen = len(wordList)\n",
    "\t\t#the graph\n",
    "\t\tdG = nx.Graph()\n",
    "\n",
    "\t\tif len(wordList)>1:\n",
    "\t\t\tpopulateGraph(wordList,dG,sliding_window)\n",
    "\t\t\tdG.remove_edges_from(dG.selfloop_edges())\n",
    "\t\t\tcentrality = nx.degree_centrality(dG) #dictionary of centralities (node:degree)\n",
    "\n",
    "\t\t\ttotalNodes += dG.number_of_nodes()\n",
    "\t\t\ttotalEdges += dG.number_of_edges()\n",
    "\t\t\t#TODO : implement comments bellow\n",
    "\t\t\t# for all nodes\n",
    "\t\t\t\t#If they are in the desired features\n",
    "\t\t\t\t\t#compute the TW-IDF score and put it in features[i,unique_words.index(g)]\n",
    "\t\t\tfor k, node_term in enumerate(dG.nodes()):\n",
    "\t\t\t\tif node_term in idf_col:\n",
    "\t\t\t\t\tfeatures[i,unique_words.index(node_term)] = centrality[node_term] * idf_col[node_term]\n",
    "\n",
    "\tif train_par:\n",
    "\t\tnodes_ret=term_num_docs.keys()\n",
    "\t\t#print \"Percentage of features kept:\"+str(feature_reduction)\n",
    "\t\tprint \"Average number of nodes:\"+str(float(totalNodes)/num_documents)\n",
    "\t\tprint \"Average number of edges:\"+str(float(totalEdges)/num_documents)\n",
    "\telse:\n",
    "\t\tnodes_ret=term_num_docs\n",
    "\t#return 1: features, 2: idf values (for the test data), 3: the list of terms \n",
    "\treturn features, idf_col, nodes_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populateGraph(wordList,dG,sliding_window):\n",
    "\t#TODO: implement this function\n",
    "\t#For each position/word in the word list:\n",
    "\t\t#add the -new- word in the graph\n",
    "\t\t#for all words -forward- within the window size\n",
    "\t\t\t#add new words as new nodes \n",
    "\t\t\t#add edges among all word within the window\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif not dG.has_node(word):\n",
    "\t\t\tdG.add_node(word)\n",
    "\t\tif k+sliding_window > len(wordList):\n",
    "\t\t\ttempW=len(wordList) - k\n",
    "\t\telse:\n",
    "\t\t\ttempW=sliding_window\n",
    "\t\tfor j in xrange(1, tempW):\n",
    "\t\t\tnext_word = wordList[k+j]\n",
    "\t\t\tdG.add_edge(word, next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords(wordList,term_num_docs):\n",
    "\tfound = set()\n",
    "\t#TODO: implement this function\n",
    "\t#add the terms from the wordlist to the term_num_docs dictionary or increase its count\n",
    "\tfor k, word in enumerate(wordList):\n",
    "\t\tif word not in found:\n",
    "\t\t\tfound.add(word)\n",
    "\t\t\tif word in term_num_docs:\n",
    "\t\t\t\tterm_num_docs[word] +=1\n",
    "\t\t\telse :\n",
    "\t\t\t\tterm_num_docs[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw_idf=createGraphFeatures(num_documents,reviews,unique_words,sliding_window,train_par,idf_learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(tw_idf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(pd.DataFrame(tw_idf[0]), path=\"/Users/Flukmacdesof/projet-MAP-670\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new features to the Tf-Idf matrix\n",
    "#### New features are : \n",
    "- Number of exclamation point\n",
    "- Number of interrogation point\n",
    "- Number of suspension point\n",
    "- Review length\n",
    "- Number of word (word_count)\n",
    "- Movie mentionned\n",
    "- Grade mentionned\n",
    "\n",
    "- Smileys (How to deal with them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def csr_vappend(a,b):\n",
    "    \"\"\" Takes in 2 csr_matrices and appends the second one to the bottom of the first one. \n",
    "    Much faster than scipy.sparse.vstack but assumes the type to be csr and overwrites\n",
    "    the first matrix instead of copying it. The data, indices, and indptr still get copied.\"\"\"\n",
    "\n",
    "    a.data = np.hstack((a.data,b.data))\n",
    "    a.indices = np.hstack((a.indices,b.indices))\n",
    "    a.indptr = np.hstack((a.indptr,(b.indptr + a.nnz)[1:]))\n",
    "    a._shape = (a.shape[0]+b.shape[0],b.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excla = scipy.sparse.csr_matrix(excla)\n",
    "inter = scipy.sparse.csr_matrix(inter)\n",
    "susp = scipy.sparse.csr_matrix(susp)\n",
    "rev_length = scipy.sparse.csr_matrix(rev_length)\n",
    "rev_word_count = scipy.sparse.csr_matrix(rev_word_count)\n",
    "rev_grade = scipy.sparse.csr_matrix(new_rev_grade)\n",
    "rev_movie = scipy.sparse.csr_matrix(new_rev_movie)\n",
    "rev_smiley = scipy.sparse.csr_matrix(features_smiley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csr_vappend(tfidf_matrix, excla)\n",
    "csr_vappend(tfidf_matrix, inter)\n",
    "csr_vappend(tfidf_matrix, susp)\n",
    "csr_vappend(tfidf_matrix,rev_length)\n",
    "csr_vappend(tfidf_matrix,rev_word_count)\n",
    "csr_vappend(tfidf_matrix,rev_grade)\n",
    "csr_vappend(tfidf_matrix,rev_movie)\n",
    "csr_vappend(tfidf_matrix, rev_smiley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CSV Creation:\n",
    "\n",
    "Create 5 csv : train_train.csv, train_test.csv, y_train_train.csv, y_train_test.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the tf-idf matrix into two data sets to process the cross validation : training and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(tfidf_matrix, Class, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tfidf_matrix.shape\n",
    "Class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data = array.data, indices = array.indices, \n",
    "             indptr = array.indptr, shape = array.shape)\n",
    "    \n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return scipy.sparse.csr_matrix(( loader['data'], loader['indices'], loader['indptr']),\n",
    "                     shape = loader['shape'])\n",
    "\n",
    "def save_csv(filename, array):\n",
    "    with open(filename, 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(array)\n",
    "        \n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter = '\\n')\n",
    "        array = [float(row[0]) for row in reader]\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('data_train', data_train)\n",
    "save_sparse_csr('data_test', data_test)\n",
    "save_csv('label_train.csv', label_train)\n",
    "save_csv('label_test.csv', label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
